RUN_AS_USER="raptor"
#	-e VLLM_ROCM_QUICK_REDUCE_QUANTIZATION=INT8 \
#	-e VLLM_ROCM_USE_AITER_RMSNORM=1 \

sudo docker run -it \
	--network=host \
	--group-add=video \
	--ipc=host \
	--cap-add=SYS_PTRACE \
	--security-opt seccomp=unconfined \
	--device /dev/kfd \
	--device /dev/dri \
	-v /etc/passwd:/etc/passwd:ro \
	-v /etc/group:/etc/group:ro \
	-e HSA_OVERRIDE_GFX_VERSION=11.0.0 \
	-e VLLM_USE_V1=1 \
	-e VLLM_USE_TRITON_FLASH_ATTN=0 \
	-e FLASH_ATTENTION_TRITON_AMD_ENABLE=TRUE \
	-e FLASH_ATTENTION_TRITON_AMD_AUTOTUNE=TRUE \
	-e TRANSFORMERS_OFFLINE=1 \
	-e PYTORCH_HIP_ALLOC_CONF=garbage_collection_threshold:0.8,max_split_size_mb:128 \
	-v $HOME:$HOME:rw \
	-u $(id -u $RUN_AS_USER):$(id -g $RUN_AS_USER) \
	$(id -G $RUN_AS_USER | perl -pe 's/(\d+)/--group-add \1/g') \
	ge-vllm-rocm \
	bash \
	--norc \
	-c \
	"vllm serve \\
        --gpu-memory-utilization=0.98 \\
        --model-impl=transformers \\
        --enable-auto-tool-choice \\
        --tool-call-parser=hermes \\
        --served-model-name=gpt-4.1 \\
        --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' \\
        --max-model-len 131072 \\
        --device=auto \\
        Qwen/Qwen3-30B-A3B"

