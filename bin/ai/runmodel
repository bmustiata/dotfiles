#!/usr/bin/env python3

import click
import textwrap
import subprocess

DEFAULT_MODEL="Qwen/Qwen2.5-7B-Instruct-1M"


@click.command(help=textwrap.dedent(f"""\
        default_model: {DEFAULT_MODEL}
    """))
@click.option("--reasoning", is_flag=True, default=False,
              help="This is a reasoning model")
@click.option("--deepseek", "--ds", "-ds", is_flag=True, default=False,
              help="If it's deepseek, we need to enable the deepseek reasoning parser")
@click.option("--model-length",
              help="How many tokens should our model ingest at most",
              default="8192")
@click.option("--served-model-name",
              help="The model name to serve (i.e. gpt-4 so other vendors are happy)")
@click.argument("model_name", default=DEFAULT_MODEL, nargs=1)
def main(
        reasoning: bool,
        deepseek: bool,
        model_name: str,
        model_length: str,
        served_model_name: str,
    ) -> None:

    reasoning_parser = ""
    if deepseek:
        reasoning_parser = "--reasoning-parser=deepseek_r1"
        reasoning = True

    enable_reasoning = ""
    if reasoning:
        enable_reasoning = "--enable-reasoning"

    served_model_name_option = ""
    if served_model_name:
        served_model_name_option=f"--served-model-name={served_model_name}"
        #--cpu-offload-gb=64 \
        #--device auto \
        # --quantization=bitsandbytes \

    command = textwrap.dedent(f"""\
        vllm serve \
        {enable_reasoning} \
        {reasoning_parser} \
        --gpu-memory-utilization=0.95 \
        --enforce-eager \
        --max-model-len={model_length} \
        --device auto \
        --quantization=bitsandbytes \
        {served_model_name_option} \
        {model_name}
    """)
    print(command)

    subprocess.check_call(["/bin/sh", "-c", command])


if __name__ == '__main__':
    main()

